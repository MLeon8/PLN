{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff81c29",
   "metadata": {},
   "source": [
    "# Analisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa9a73",
   "metadata": {},
   "source": [
    "Creacion de un analizador de sentimientos a partir de texto, usando el conjunto de datos https://ai.stanford.edu/~amaas/data/sentiment/. Este conjunto consta de 2 carpetas, train y test, cada una de las cuales poseen a su vez dos carpetas, pos y neg, las que almacenan comentarios de peliculas, segun recuerdo, positivos y negativos respectivamente. La idea es entrenar un modelo que sea capaz de predecir si los comentarios son positivos o negativos. Teniendo en cuenta que las categorias de los comentarios son positivos o negativos, se hace uso de regresion logistica, la cual a su vez hace uso de la funcion sigmoide. La funcion sigmoide, segun recuerdo, tiene la forma f(x) = 1/ [1 + e^(-t) ], donde t en este caso sera el vector de palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c459f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las bibliotecas basicas\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fe5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la carpeta donde estan los conjuntos de entrenamiento y prueba respectivamente\n",
    "train_folder, test_folder = './aclImdb/train/', './aclImdb/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creamos el conjunto de datos. Es decir, para el conjunto de entrenamiento, tomaremos todos los comentarios positivos y los\n",
    "almacenaremos juntos y etiquetaremos como 1, por otra parte con los comentarios negativos igualmente los almacenaremos en\n",
    "donde mismo pusimos los comentarios positivos del conjunto de entrenamiento, pero en este caso los etiquetaremos como 0.\n",
    "\"\"\"\n",
    "def creatingDataSet(folder, folder_type = ['pos/', 'neg/'], result_file_path = None):\n",
    "    data = []\n",
    "    \n",
    "    for f_type in folder_type:\n",
    "        temp_folder=folder + f_type\n",
    "        print(temp_folder)\n",
    "        folder_class = 1\n",
    "        if f_type != 'pos/':\n",
    "            folder_class = 0\n",
    "        #Listando los archivos\n",
    "        files = os.listdir(temp_folder)\n",
    "        #Analizando los archivos\n",
    "        for file in files:\n",
    "            with open(temp_folder+file, \"r\", encoding=\"utf8\") as f:\n",
    "                value = f.read()\n",
    "                data.append( [value, folder_class] )\n",
    "    \n",
    "    df = pd.DataFrame( data = data, columns = ['text', 'class'])\n",
    "    if result_file_path:\n",
    "        df.to_csv(result_file_path, index = False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando archivos CSV donde se unifican los datos de entrenamiento, asi como con los datos de prueba\n",
    "creatingDataSet('./aclImdb/train/', result_file_path='./train.csv')\n",
    "creatingDataSet('./aclImdb/test/', result_file_path='./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e641619",
   "metadata": {},
   "source": [
    "#  Vectorizando los conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b00e75",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "En esta fase convertiremos cada uno de los textos a vector, es decir a numeros, para su posterior analisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv('./train.csv')\n",
    "test_dataset = pd.read_csv('./test.csv')\n",
    "test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c232e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Importamos la biblioteca que utilizaremos para la vectorizacion de las palabras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e188554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Descartamos las palabras cuya frecuencia es menor a .1 o superior a .5; es decir, descartamos las palabras con una baja\n",
    "frecuencia, pues estas suelen ser errores ortograficos. Asimismo descartamos las palabras con una alta frecuencia, pues \n",
    "estas suelen ser preposiciones, articulos y conjunciones.\n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer(min_df = .1, max_df = .5, ngram_range=(1, 3), stop_words = 'english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Obteniendo los textos de entrenamiento y prueba a vectorizar\n",
    "train_dataset_texts = train_dataset['text'].values\n",
    "test_dataset_texts = test_dataset['text'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Transformando los textos de entrenamiento\n",
    "features = tfidf.fit_transform(train_dataset_texts)\n",
    "#Ahora el text \n",
    "print(train_dataset_texts[0])\n",
    "#se traduce en\n",
    "print(features.todense()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f13b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obteniendo las clases del conjunto de entrenamiento\n",
    "train_dataset_class = train_dataset['class'].values\n",
    "#Transformando las clases del conjunto de entrenamiento a vector columna\n",
    "train_dataset_class = train_dataset_class.reshape( -1, 1)\n",
    "print(train_dataset_class.shape)\n",
    "#Transformando la matriz dispersa a matriz\n",
    "train_mtx = features.todense()\n",
    "print(train_mtx.shape)\n",
    "train_mtx_array = np.array( train_mtx )\n",
    "train_mtx_array = np.hstack( (train_mtx_array, train_dataset_class) )\n",
    "print(train_mtx_array.shape)\n",
    "\n",
    "#Exportando el dataframe\n",
    "columns = tfidf.get_feature_names_out()\n",
    "columns = columns.tolist()\n",
    "columns.append('class')\n",
    "\n",
    "df = pd.DataFrame( data = train_mtx_array, columns = columns)\n",
    "df.to_csv('train_vector.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = tfidf.transform(test_dataset_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obteniendo las clases del conjunto de prueba\n",
    "test_dataset_class = test_dataset['class'].values\n",
    "#Transformando las clases del conjunto de prueba a vector columna\n",
    "test_dataset_class = test_dataset_class.reshape( -1, 1)\n",
    "print(test_dataset_class.shape)\n",
    "#Transformando la matriz dispersa a matriz\n",
    "test_mtx = features.todense()\n",
    "print(test_mtx.shape)\n",
    "test_mtx_array = np.array( test_mtx )\n",
    "test_mtx_array = np.hstack( (test_mtx_array, test_dataset_class) )\n",
    "print(test_mtx_array.shape)\n",
    "\n",
    "#Exportando el dataframe\n",
    "columns = tfidf.get_feature_names_out()\n",
    "columns = columns.tolist()\n",
    "columns.append('class')\n",
    "\n",
    "df = pd.DataFrame( data = test_mtx_array, columns = columns)\n",
    "df.to_csv('test_vector.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae896310",
   "metadata": {},
   "source": [
    " # Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0e2b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una vez los textos estan vectorizados paramos a la creacion del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b78321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Leemos los conjuntos de datos de entrenamiento y prueba\n",
    "train_dataset = pd.read_csv('./train_vector.csv')\n",
    "test_dataset = pd.read_csv('./test_vector.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos los vectores de los comentarios, es decir, se ignora la ultima columna pues esta indica si el comentario fue\n",
    "#positivo o negativo\n",
    "X_train = train_dataset.iloc[:,:-1]\n",
    "print(X_train.shape)\n",
    "#Obtenemos la etiqueta de los comentarios\n",
    "y_train = train_dataset['class'].values\n",
    "print(y_train)\n",
    "\n",
    "X_test = test_dataset.iloc[:,:-1]\n",
    "print(X_test.shape)\n",
    "y_test = test_dataset['class'].values\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el modelo de regresion logistica\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de entrenamiento\n",
    "print('Train Score', clf.score(X_train, y_train))\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de prueba\n",
    "print('Test Score', clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c4f2c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Como vemos tenemos cerca de un 70 porciento de exactitud al etiquetar los comentarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e33b7",
   "metadata": {},
   "source": [
    "# Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f145f01",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sin embargo, la anterior estrategia de vectorizacion no resulta del todo adecuada cuando trabajamos con conjuntos de datos demasiado extensos, en el orden de los terabytes, es decir, cuando el numero de caracteristicas es demasiado grande. En este momento una mejor estrategia es vectorizar las caracteristicas, de forma tal que para cada palabra cada caracter es multiplicado por un determinado numero primo elevado a la i-esima potencia, para i=0...no_caracteres - 1. Es decir, el hash de una cadena cualquiera es: hash(s) = s[0] + s[1] p^{1} + ... + s[n] p^{n-1}. Ahora vemos un ejemplo de lo anteriormente expuesto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0252ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "X.todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4f42c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Creamos un vectorizador, el cual aplicara una funcion hash a cada palabra, ademas esta tabla hash constara de 1024 elementos, lo que si bien pudiese provocar que existan colisiones y disminuya la exactitud del modelo, en este caso no es asi. Aunque debemos recordar que en caso de que lo deseemos podemos aumentar el numero de caracteristicas hasta 2^20, lo que disminuye la posibilidad de que ocurran colisiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=2**10, stop_words = 'english')\n",
    "features = vectorizer.fit_transform(train_dataset_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Obteniendo las clases del conjunto de entrenamiento\n",
    "train_dataset_class = train_dataset['class'].values\n",
    "#Transformando la matriz dispersa a matriz\n",
    "train_mtx = features.todense()\n",
    "print(train_mtx.shape)\n",
    "train_mtx_array = np.array( train_mtx )\n",
    "\n",
    "#Las 1024 caracteristicas que surgen son resultados de la aplicacion de una funcion hash, por lo que no se puede hacer el \n",
    "#proceso inverso, es decir, a partir de valor hash recuperar la caracteristica a la que esta asociada, pues en realidad \n",
    "#no es del todo asi. Motivo por el cual no se exporto los dataframe que resultaron de la vectorizacion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = vectorizer.transform(test_dataset_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Obteniendo las clases del conjunto de prueba\n",
    "test_dataset_class = test_dataset['class'].values\n",
    "#Transformando las clases del conjunto de prueba a vector columna\n",
    "test_dataset_class = test_dataset_class.reshape( -1, 1)\n",
    "print(test_dataset_class.shape)\n",
    "#Transformando la matriz dispersa a matriz\n",
    "test_mtx = features.todense()\n",
    "print(test_mtx.shape)\n",
    "test_mtx_array = np.array( test_mtx )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_mtx_array\n",
    "y_train = train_dataset_class\n",
    "\n",
    "X_test = test_mtx_array\n",
    "y_test = test_dataset_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f51857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el modelo de regresion logistica\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de entrenamiento\n",
    "print('Train Score', clf.score(X_train, y_train))\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de prueba\n",
    "print('Test Score', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b864a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
