{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff81c29",
   "metadata": {},
   "source": [
    "# Analisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa9a73",
   "metadata": {},
   "source": [
    "Creacion de un analizador de sentimientos a partir de texto, usando el conjunto de datos https://ai.stanford.edu/~amaas/data/sentiment/. Este conjunto consta de 2 carpetas, train y test, cada una de las cuales poseen a su vez dos carpetas, pos y neg, las que almacenan comentarios de peliculas, segun recuerdo, positivos y negativos respectivamente. La idea es entrenar un modelo que sea capaz de predecir si los comentarios son positivos o negativos. Teniendo en cuenta que las categorias de los comentarios son positivos o negativos, se hace uso de regresion logistica, la cual a su vez hace uso de la funcion sigmoide. La funcion sigmoide, segun recuerdo, tiene la forma f(x) = 1/ [1 + e^(-t) ], donde t en este caso sera el vector de palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56c459f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las bibliotecas basicas\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "368fe5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la carpeta donde estan los conjuntos de entrenamiento y prueba respectivamente\n",
    "train_folder, test_folder,encuesta_folder = '/home/mleon/Escritorio/PLN/aclImdb/train/', '/home/mleon/Escritorio/PLN/aclImdb/test/','/home/mleon/Escritorio/PLN/aclImdb/encuesta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7c3695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creamos el conjunto de datos. Es decir, para el conjunto de entrenamiento, tomaremos todos los comentarios positivos y los\n",
    "almacenaremos juntos y etiquetaremos como 1, por otra parte con los comentarios negativos igualmente los almacenaremos en\n",
    "donde mismo pusimos los comentarios positivos del conjunto de entrenamiento, pero en este caso los etiquetaremos como 0.\n",
    "\"\"\"\n",
    "def creatingDataSet(folder, folder_type = ['pos/', 'neg/'], result_file_path = None):\n",
    "    data = []\n",
    "    \n",
    "    for f_type in folder_type:\n",
    "        temp_folder=folder + f_type\n",
    "        print(temp_folder)\n",
    "        folder_class = 1\n",
    "        if f_type != 'pos/':\n",
    "            folder_class = 0\n",
    "        #Listando los archivos\n",
    "        files = os.listdir(temp_folder)\n",
    "        #Analizando los archivos\n",
    "        for file in files:\n",
    "            with open(temp_folder+file, \"r\", encoding=\"utf8\") as f:\n",
    "                value = f.read()\n",
    "                data.append( [value, folder_class] )\n",
    "    \n",
    "    df = pd.DataFrame( data = data, columns = ['text', 'class'])\n",
    "    if result_file_path:\n",
    "        df.to_csv(result_file_path, index = False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e1f7b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mleon/Escritorio/PLN/aclImdb/train/pos/\n",
      "/home/mleon/Escritorio/PLN/aclImdb/train/neg/\n",
      "/home/mleon/Escritorio/PLN/aclImdb/test/pos/\n",
      "/home/mleon/Escritorio/PLN/aclImdb/test/neg/\n",
      "/home/mleon/Escritorio/PLN/aclImdb/encuesta/pos/\n",
      "/home/mleon/Escritorio/PLN/aclImdb/encuesta/neg/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked the interview, I felt like in American...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was difficult because I feel that I lack ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class\n",
       "0  I liked the interview, I felt like in American...      1\n",
       "1  It was difficult because I feel that I lack ti...      0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creando archivos CSV donde se unifican los datos de entrenamiento, asi como con los datos de prueba\n",
    "creatingDataSet('/home/mleon/Escritorio/PLN/aclImdb/train/', result_file_path='/home/mleon/Escritorio/PLN/aclImdb/train.csv')\n",
    "creatingDataSet('/home/mleon/Escritorio/PLN/aclImdb/test/', result_file_path='/home/mleon/Escritorio/PLN/aclImdb/test.csv')\n",
    "creatingDataSet('/home/mleon/Escritorio/PLN/aclImdb/encuesta/', result_file_path='/home/mleon/Escritorio/PLN/aclImdb/encuesta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e641619",
   "metadata": {},
   "source": [
    "#  Vectorizando los conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b00e75",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "En esta fase convertiremos cada uno de los textos a vector, es decir a numeros, para su posterior analisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5f7505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = pd.read_csv('/home/mleon/Escritorio/PLN/aclImdb/train.csv')\n",
    "test_dataset = pd.read_csv('/home/mleon/Escritorio/PLN/aclImdb/test.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c232e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Importamos la biblioteca que utilizaremos para la vectorizacion de las palabras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e188554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Descartamos las palabras cuya frecuencia es menor a .1 o superior a .5; es decir, descartamos las palabras con una baja\n",
    "frecuencia, pues estas suelen ser errores ortograficos. Asimismo descartamos las palabras con una alta frecuencia, pues \n",
    "estas suelen ser preposiciones, articulos y conjunciones.\n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer(min_df = .1, max_df = .5, ngram_range=(1, 3), stop_words = 'english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136d0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Obteniendo los textos de entrenamiento y prueba a vectorizar\n",
    "train_dataset_texts = train_dataset['text'].values\n",
    "test_dataset_texts = test_dataset['text'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8294ba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This very strange movie is unlike anything made in the west at the time. With its tumultuous emotions and net of visions, dreams, and startling images, its effect is both beautiful and unsettling. The actors are choreographed more like dance than acting. It contains the only dream sequence I know of that actually resembles a real nightmare (sorry, Dali fans).\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.21936995 0.21527247\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22471577 0.         0.         0.         0.\n",
      "  0.         0.         0.3244169  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21935281\n",
      "  0.         0.49510277 0.         0.         0.19976217 0.\n",
      "  0.         0.         0.         0.50645754 0.         0.\n",
      "  0.         0.         0.         0.         0.24136148 0.18167004\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26245978 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Transformando los textos de entrenamiento\n",
    "features = tfidf.fit_transform(train_dataset_texts)\n",
    "#Ahora el text \n",
    "print(train_dataset_texts[1000])\n",
    "#se traduce en\n",
    "print(features.todense()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f13b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25002, 1)\n",
      "(25002, 77)\n",
      "(25002, 78)\n"
     ]
    }
   ],
   "source": [
    "#Obteniendo las clases del conjunto de entrenamiento\n",
    "train_dataset_class = train_dataset['class'].values\n",
    "#Transformando las clases del conjunto de entrenamiento a vector columna\n",
    "train_dataset_class = train_dataset_class.reshape( -1, 1)\n",
    "print(train_dataset_class.shape)\n",
    "#Transformando la matriz dispersa a matriz\n",
    "train_mtx = features.todense()\n",
    "print(train_mtx.shape)\n",
    "train_mtx_array = np.array( train_mtx )\n",
    "train_mtx_array = np.hstack( (train_mtx_array, train_dataset_class) )\n",
    "print(train_mtx_array.shape)\n",
    "\n",
    "#Exportando el dataframe\n",
    "columns = tfidf.get_feature_names_out()\n",
    "columns = columns.tolist()\n",
    "columns.append('class')\n",
    "\n",
    "df = pd.DataFrame( data = train_mtx_array, columns = columns)\n",
    "df.to_csv('/home/mleon/Escritorio/PLN/aclImdb/train_vector.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eea5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = tfidf.transform(test_dataset_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea88798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25002, 1)\n",
      "(25002, 77)\n",
      "(25002, 78)\n"
     ]
    }
   ],
   "source": [
    "#Obteniendo las clases del conjunto de prueba\n",
    "test_dataset_class = test_dataset['class'].values\n",
    "#Transformando las clases del conjunto de prueba a vector columna\n",
    "test_dataset_class = test_dataset_class.reshape( -1, 1)\n",
    "print(test_dataset_class.shape)\n",
    "#Transformando la matriz dispersa a matriz\n",
    "test_mtx = features.todense()\n",
    "print(test_mtx.shape)\n",
    "test_mtx_array = np.array( test_mtx )\n",
    "test_mtx_array = np.hstack( (test_mtx_array, test_dataset_class) )\n",
    "print(test_mtx_array.shape)\n",
    "\n",
    "#Exportando el dataframe\n",
    "columns = tfidf.get_feature_names_out()\n",
    "columns = columns.tolist()\n",
    "columns.append('class')\n",
    "\n",
    "df = pd.DataFrame( data = test_mtx_array, columns = columns)\n",
    "df.to_csv('/home/mleon/Escritorio/PLN/aclImdb/test_vector.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae896310",
   "metadata": {},
   "source": [
    " # Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0e2b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una vez los textos estan vectorizados paramos a la creacion del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b78321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Leemos los conjuntos de datos de entrenamiento y prueba\n",
    "train_dataset = pd.read_csv('/home/mleon/Escritorio/PLN/aclImdb/train_vector.csv')\n",
    "test_dataset = pd.read_csv('/home/mleon/Escritorio/PLN/aclImdb//test_vector.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b663862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25002, 77)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "(25002, 77)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos los vectores de los comentarios, es decir, se ignora la ultima columna pues esta indica si el comentario fue\n",
    "#positivo o negativo\n",
    "X_train = train_dataset.iloc[:,:-1]\n",
    "print(X_train.shape)\n",
    "#Obtenemos la etiqueta de los comentarios\n",
    "y_train = train_dataset['class'].values\n",
    "print(y_train)\n",
    "\n",
    "X_test = test_dataset.iloc[:,:-1]\n",
    "print(X_test.shape)\n",
    "y_test = test_dataset['class'].values\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1225d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score 0.7173826093912486\n",
      "Test Score 0.7156627469802416\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo de regresion logistica\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de entrenamiento\n",
    "print('Train Score', clf.score(X_train, y_train))\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de prueba\n",
    "print('Test Score', clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c4f2c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Como vemos tenemos cerca de un 70 porciento de exactitud al etiquetar los comentarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e33b7",
   "metadata": {},
   "source": [
    "# Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f145f01",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sin embargo, la anterior estrategia de vectorizacion no resulta del todo adecuada cuando trabajamos con conjuntos de datos demasiado extensos, en el orden de los terabytes, es decir, cuando el numero de caracteristicas es demasiado grande. En este momento una mejor estrategia es vectorizar las caracteristicas, de forma tal que para cada palabra cada caracter es multiplicado por un determinado numero primo elevado a la i-esima potencia, para i=0...no_caracteres - 1. Es decir, el hash de una cadena cualquiera es: hash(s) = s[0] + s[1] p^{1} + ... + s[n] p^{n-1}. Ahora vemos un ejemplo de lo anteriormente expuesto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0252ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-0.57735027,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.57735027,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.57735027,  0.        ,\n",
       "          0.        ],\n",
       "        [-0.81649658,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.40824829,  0.        ,  0.40824829,  0.        ,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        , -0.70710678,\n",
       "          0.70710678,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ],\n",
       "        [-0.57735027,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.57735027,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.57735027,  0.        ,\n",
       "          0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)\n",
    "X.todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4f42c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Creamos un vectorizador, el cual aplicara una funcion hash a cada palabra, ademas esta tabla hash constara de 1024 elementos, lo que si bien pudiese provocar que existan colisiones y disminuya la exactitud del modelo, en este caso no es asi. Aunque debemos recordar que en caso de que lo deseemos podemos aumentar el numero de caracteristicas hasta 2^20, lo que disminuye la posibilidad de que ocurran colisiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8bb0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=2**10, stop_words = 'english')\n",
    "features = vectorizer.fit_transform(train_dataset_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "955e0ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25002, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Obteniendo las clases del conjunto de entrenamiento\n",
    "train_dataset_class = train_dataset['class'].values\n",
    "#Transformando la matriz dispersa a matriz\n",
    "train_mtx = features.todense()\n",
    "print(train_mtx.shape)\n",
    "train_mtx_array = np.array( train_mtx )\n",
    "\n",
    "#Las 1024 caracteristicas que surgen son resultados de la aplicacion de una funcion hash, por lo que no se puede hacer el \n",
    "#proceso inverso, es decir, a partir de valor hash recuperar la caracteristica a la que esta asociada, pues en realidad \n",
    "#no es del todo asi. Motivo por el cual no se exporto los dataframe que resultaron de la vectorizacion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b3c8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = vectorizer.transform(test_dataset_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78ec4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25002, 1)\n",
      "(25002, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Obteniendo las clases del conjunto de prueba\n",
    "test_dataset_class = test_dataset['class'].values\n",
    "#Transformando las clases del conjunto de prueba a vector columna\n",
    "test_dataset_class = test_dataset_class.reshape( -1, 1)\n",
    "print(test_dataset_class.shape)\n",
    "#Transformando la matriz dispersa a matriz\n",
    "test_mtx = features.todense()\n",
    "print(test_mtx.shape)\n",
    "test_mtx_array = np.array( test_mtx )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f8b3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_mtx_array\n",
    "y_train = train_dataset_class\n",
    "\n",
    "X_test = test_mtx_array\n",
    "y_test = test_dataset_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f51857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score 0.8250939924806016\n",
      "Test Score 0.7993360531157507\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo de regresion logistica\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de entrenamiento\n",
    "print('Train Score', clf.score(X_train, y_train))\n",
    "#Evaluamos la exactirud del modelo para etiquetar los comentarios presentes en el conjunto de prueba\n",
    "print('Test Score', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c63f7",
   "metadata": {},
   "source": [
    "# Aplicandolo a las encuestas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b37811c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Encuesta_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/mleon/Escritorio/PLN/analisis/Respuestas/Guadalajara_Respuestas.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m----> 3\u001b[0m Encuesta_dataset_texts \u001b[38;5;241m=\u001b[39m \u001b[43mEncuesta_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "Encuesta_dataset = pd.read_csv('/home/mleon/Escritorio/PLN/analisis/Respuestas/Guadalajara_Respuestas.csv')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Encuesta_dataset_texts = Encuesta_dataset['text'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77b2199b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Encuesta_dataset_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Transformando los textos \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m features \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mEncuesta_dataset_texts\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Ahora el text \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(Encuesta_dataset_texts[\u001b[38;5;241m1000\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Encuesta_dataset_texts' is not defined"
     ]
    }
   ],
   "source": [
    "#Transformando los textos \n",
    "features = tfidf.fit_transform(Encuesta_dataset_texts)\n",
    "#Ahora el text \n",
    "print(Encuesta_dataset_texts[1000])\n",
    "#se traduce en\n",
    "print(features.todense()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2af09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2912de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
