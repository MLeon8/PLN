{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4813459f",
   "metadata": {},
   "source": [
    "# Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e48719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ac43f",
   "metadata": {},
   "source": [
    "We want to create a network that has only one LSTM cell. We have to pass 2 elements to LSTM, the <b>prv_output</b> and <b>prv_state</b>, so called, <b>h</b> and <b>c</b>. Therefore, we initialize a state vector, <b>state</b>.  Here, <b>state</b> is a tuple with 2 elements, each one is of size \\[1 x 4], one for passing prv_output to next time step, and another for passing the prv_state to next time stamp.\n",
    "\n",
    "\\\\\n",
    "\n",
    "Queremos crear una red que tenga solo una celda LSTM. Tenemos que pasar 2 elementos a LSTM, prv_output y prv_state, llamados h y c. Por lo tanto, inicializamos un vector de estado, state. Aquí, state es una tupla con 2 elementos, cada uno de tamaño [1 x 4], uno para pasar prv_output al siguiente paso de tiempo y otro para pasar prv_state al siguiente sello de tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818ee98c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n",
    "\n",
    "state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40934d01",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "lstm = tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)\n",
    "\n",
    "lstm.states=state\n",
    "\n",
    "#As we can see, the states has 2 parts, the new state c, and also the output h.\n",
    "print(lstm.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe74bd",
   "metadata": {},
   "source": [
    "Let define a sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a055eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n",
    "\n",
    "batch_size = 1\n",
    "sentence_max_length = 1\n",
    "n_features = 6\n",
    "\n",
    "new_shape = (batch_size, sentence_max_length, n_features)\n",
    "\n",
    "inputs = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a31ee",
   "metadata": {},
   "source": [
    "Now, we can pass the input to lstm_cell, and check the new state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f9363dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  tf.Tensor([1 1 4], shape=(3,), dtype=int32)\n",
      "Output : tf.Tensor([[[-0.15524554  0.0623856  -0.02357852  0.31590742]]], shape=(1, 1, 4), dtype=float32)\n",
      "Memory shape:  tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "Memory :  tf.Tensor([[-0.15524554  0.0623856  -0.02357852  0.31590742]], shape=(1, 4), dtype=float32)\n",
      "Carry state shape:  tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "Carry state:  tf.Tensor([[-0.25178537  0.07693953 -0.09550406  0.6387666 ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "\n",
    "print('Output shape: ', tf.shape(output))\n",
    "print('Output :', output)\n",
    "\n",
    "print('Memory shape: ', tf.shape(final_memory_state))\n",
    "print('Memory : ', final_memory_state)\n",
    "\n",
    "print('Carry state shape: ', tf.shape(final_carry_state))\n",
    "print('Carry state: ', final_carry_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaaf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60956e3",
   "metadata": {},
   "source": [
    "What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87562707",
   "metadata": {},
   "source": [
    "Lets create the stacked LSTM cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f4b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc98ccc",
   "metadata": {},
   "source": [
    "Creating the first layer LTSM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed9eede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n",
    "cell1 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_1)\n",
    "cells.append(cell1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a346b",
   "metadata": {},
   "source": [
    "Creating the second layer LTSM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a450ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n",
    "cell2 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_2)\n",
    "cells.append(cell2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b803c2",
   "metadata": {},
   "source": [
    "To create a multi-layer LTSM we use the <b>tf.keras.layers.StackedRNNCells</b> function, it takes in multiple single layer LTSM cells to create a multilayer stacked LTSM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4781ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm =  tf.keras.layers.StackedRNNCells(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec91daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create the RNN from stacked_lstm:\n",
    "lstm_layer= tf.keras.layers.RNN(stacked_lstm ,return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4dce9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n",
    "sample_input\n",
    "\n",
    "batch_size = 2\n",
    "time_steps = 3\n",
    "features = 6\n",
    "new_shape = (batch_size, time_steps, features)\n",
    "\n",
    "x = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb6f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final_memory_state, final_carry_state  = lstm_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cafb862a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  tf.Tensor([2 3 5], shape=(3,), dtype=int32)\n",
      "Output :  tf.Tensor(\n",
      "[[[-0.02477117 -0.00665748 -0.01562491 -0.02800175  0.05869427]\n",
      "  [-0.06074494  0.00439783 -0.02671911 -0.08622565  0.13191596]\n",
      "  [-0.09918674  0.01481524 -0.04344293 -0.13013206  0.18972598]]\n",
      "\n",
      " [[-0.02477117 -0.00665748 -0.01562491 -0.02800175  0.05869428]\n",
      "  [-0.05941437  0.01052649 -0.0226018  -0.09334762  0.13623793]\n",
      "  [-0.06573982  0.00519138 -0.03357539 -0.08612452  0.15062016]]], shape=(2, 3, 5), dtype=float32)\n",
      "Memory shape:  tf.Tensor([2 2 4], shape=(3,), dtype=int32)\n",
      "Memory :  [<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 0.78775716, -0.37040275,  0.30532894,  0.03228419],\n",
      "       [ 0.38800165, -0.12782474,  0.14623934,  0.13303787]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[ 1.4996562 , -1.1974225 ,  0.78079283,  0.24307354],\n",
      "       [ 1.5502831 , -0.6031351 ,  1.0484042 ,  0.28928846]],\n",
      "      dtype=float32)>]\n",
      "Carry state shape:  tf.Tensor([2 2 5], shape=(3,), dtype=int32)\n",
      "Carry state :  [<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
      "array([[-0.09918674,  0.01481524, -0.04344293, -0.13013206,  0.18972598],\n",
      "       [-0.06573982,  0.00519138, -0.03357539, -0.08612452,  0.15062016]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
      "array([[-0.19243328,  0.02572563, -0.09459467, -0.25306556,  0.41844252],\n",
      "       [-0.1297673 ,  0.00942773, -0.06813988, -0.16150631,  0.32128775]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print('Output shape: ', tf.shape(output))\n",
    "print('Output : ', output)\n",
    "\n",
    "print('Memory shape: ', tf.shape(final_memory_state))\n",
    "print('Memory : ', final_memory_state)\n",
    "\n",
    "print('Carry state shape: ', tf.shape(final_carry_state))\n",
    "print('Carry state : ', final_carry_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87555a",
   "metadata": {},
   "source": [
    "As you see, the output is of shape (2, 3, 5), which corresponds to our 2 batches, 3 elements in our sequence, and the dimensionality of the output which is 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
