{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4813459f",
   "metadata": {},
   "source": [
    "# Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f4970",
   "metadata": {},
   "source": [
    "We want to create a network that has only one LSTM cell. We have to pass 2 elements to LSTM, the <b>prv_output</b> and <b>prv_state</b>, so called, <b>h</b> and <b>c</b>. Therefore, we initialize a state vector, <b>state</b>.  Here, <b>state</b> is a tuple with 2 elements, each one is of size \\[1 x 4], one for passing prv_output to next time step, and another for passing the prv_state to next time stamp.\n",
    "\n",
    "\\\\\n",
    "\n",
    "Queremos crear una red que tenga solo una celda LSTM. Tenemos que pasar 2 elementos a LSTM, prv_output y prv_state, llamados h y c. Por lo tanto, inicializamos un vector de estado, state. Aquí, state es una tupla con 2 elementos, cada uno de tamaño [1 x 4], uno para pasar prv_output al siguiente paso de tiempo y otro para pasar prv_state al siguiente sello de tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ee98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n",
    "\n",
    "state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40934d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)\n",
    "\n",
    "lstm.states=state\n",
    "\n",
    "#As we can see, the states has 2 parts, the new state c, and also the output h.\n",
    "print(lstm.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35309dcd",
   "metadata": {},
   "source": [
    "Let define a sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a055eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n",
    "\n",
    "batch_size = 1\n",
    "sentence_max_length = 1\n",
    "n_features = 6\n",
    "\n",
    "new_shape = (batch_size, sentence_max_length, n_features)\n",
    "\n",
    "inputs = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751be0a",
   "metadata": {},
   "source": [
    "Now, we can pass the input to lstm_cell, and check the new state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9363dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "\n",
    "print('Output shape: ', tf.shape(output))\n",
    "print('Output :', output)\n",
    "\n",
    "print('Memory shape: ', tf.shape(final_memory_state))\n",
    "print('Memory : ', final_memory_state)\n",
    "\n",
    "print('Carry state shape: ', tf.shape(final_carry_state))\n",
    "print('Carry state: ', final_carry_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597276b",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ad290",
   "metadata": {},
   "source": [
    "What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0a653",
   "metadata": {},
   "source": [
    "Lets create the stacked LSTM cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defdf2cc",
   "metadata": {},
   "source": [
    "Creating the first layer LTSM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9eede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n",
    "cell1 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_1)\n",
    "cells.append(cell1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7626a",
   "metadata": {},
   "source": [
    "Creating the second layer LTSM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a450ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n",
    "cell2 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_2)\n",
    "cells.append(cell2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd81d0",
   "metadata": {},
   "source": [
    "To create a multi-layer LTSM we use the <b>tf.keras.layers.StackedRNNCells</b> function, it takes in multiple single layer LTSM cells to create a multilayer stacked LTSM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm =  tf.keras.layers.StackedRNNCells(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create the RNN from stacked_lstm:\n",
    "lstm_layer= tf.keras.layers.RNN(stacked_lstm ,return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dce9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n",
    "sample_input\n",
    "\n",
    "batch_size = 2\n",
    "time_steps = 3\n",
    "features = 6\n",
    "new_shape = (batch_size, time_steps, features)\n",
    "\n",
    "x = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final_memory_state, final_carry_state  = lstm_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output shape: ', tf.shape(output))\n",
    "print('Output : ', output)\n",
    "\n",
    "print('Memory shape: ', tf.shape(final_memory_state))\n",
    "print('Memory : ', final_memory_state)\n",
    "\n",
    "print('Carry state shape: ', tf.shape(final_carry_state))\n",
    "print('Carry state : ', final_carry_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f193ee4",
   "metadata": {},
   "source": [
    "As you see, the output is of shape (2, 3, 5), which corresponds to our 2 batches, 3 elements in our sequence, and the dimensionality of the output which is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
